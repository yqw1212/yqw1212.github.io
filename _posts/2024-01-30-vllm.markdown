---
layout: post
title:  Efficient Memory Management for Large Language Model Serving with PagedAttention
date:   2024-01-30 00:08:01 +0300
image:  2024-01-30-cyber.jpg
tags:   [paper]
---

# 基于页面注意力的大语言模型的高效内存管理**

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng Cody Hao Yu Joseph E. Gonzalez, Hao Zhang, Ion Stoica

# **摘要**

大语言模型(LLM)的高吞吐量服务需要一次批处理足够多的请求。然而，现有的系统很困难，因为每个请求的键值缓存(KV缓存)内存很大，并且动态地增长和缩小。当管理效率低下时，这些内存可能会被碎片和冗余复制严重浪费，从而限制了批处理大小。为了解决这个问题，我们提出了PagedAttention，这是一种受操作系统中经典虚拟内存和分页技术启发的注意力算法。在此基础上，我们构建了vLLM，这是一个LLM服务系统，它实现了(1)KV缓存几乎为零的浪费，(2)在请求内部和请求之间灵活地共享KV缓存，以进一步减少内存使用。我们的评估表明，与FasterTransformer和Orca等最先进的系统相比，在相同的延迟水平下，vLLM将流行LLM的吞吐量提高了2-4倍。对于更长的序列、更大的模型和更复杂的解码算法，改进更加明显。vLLM的源代码可在https://github.com/vllm-project/vllm上公开获得。

# **1介绍**

像GPT[5,37]和PaLM[9]这样的大型语言模型(LLM)的出现使编程助理[6,18]和通用聊天机器人[19,35]等新应用成为可能，它们开始深刻地影响我们的工作和日常生活。许多云计算公司[34,44]正在竞相提供这些应用程序作为托管服务。然而，运行这些应用程序是非常昂贵的，需要大量的硬件加速器，如GPU。根据最近的估计，处理LLM请求的成本可能是传统关键字查询的10倍[43]。

考虑到这些高成本，增加LLM服务系统的吞吐量(从而降低每个请求的成本)变得更加重要。

LLM的核心是自回归的Transformer模型[53]。该模型基于输入(提示)和迄今为止生成的输出标记的前一个序列，一次生成一个单词(标记)。对于每个请求，重复这个昂贵的过程，直到模型输出一个终止令牌。这种顺序生成过程使工作负载受到内存限制，使GPU的计算能力得不到充分利用，并限制了服务吞吐量。

通过将多个请求批处理在一起，可以提高吞吐量。但是，为了批量处理许多请求，应该有效地管理每个请求的内存空间。例如，图1(左)说明了在具有40GB RAM的NVIDIA A100 GPU上13B参数LLM的内存分布。大约65%的内存分配给模型权重，这些权重在服务期间保持静态。接近30%的内存用于存储请求的动态状态。对于Transformer，这些状态由与注意力机制相关的键和值张量组成，通常被称为KV缓存[41]，它表示来自早期Token的上下文，以按顺序生成新的输出Token。剩余的一小部分内存用于其他数据，包括激活——在评估LLM时创建的短暂张量。由于模型权重是恒定的，并且激活仅占用GPU内存的一小部分，因此管理KV缓存的方式对于确定最大批大小至关重要。当管理效率低下时，KV高速缓存可以显著限制批处理大小，从而限制LLM的吞吐量，如图1(右)所示。

在本文中，**我们观察到现有的LLM服务系统[31,60]无法有效地管理KV缓存。这主要是因为它们将请求的KV缓存存储在连续内存空间中，因为大多数深度学习框架[33,39]要求将张量存储在连续内存中。然而，与传统深度学习工作负载中的张量不同，KV缓存具有独特的特征:随着模型生成新的令牌，它会随着时间的推移动态增长和缩小，并且它的生命周期和长度是未知的。**这些特点使现有系统的方法在两个方面显着低效:

**首先，现有系统[31,60]存在内部和外部内存碎片。为了在连续空间中存储请求的KV缓存，它们预先分配了一个具有请求最大长度的连续内存块(例如，2048个Token)。这可能导致严重的内部碎片，因为请求的实际长度可能比它的最大长度短得多(例如，图11)。此外，即使预先知道实际长度，预分配仍然是低效的:由于在请求的生命周期内保留了整个块，其他较短的请求不能利用当前未使用的块的任何部分。此外，外部内存碎片也很重要，因为每个请求的预分配大小可能不同。**事实上，我们在图2中的分析结果显示，在现有系统中，只有20.4% - 38.2%的KV缓存内存用于存储实际令牌状态。

其次，现有系统无法利用内存共享的机会。LLM服务通常使用高级解码算法，例如并行采样和波束搜索，每个请求生成多个输出。在这些场景中，请求由多个序列组成，这些序列可以部分共享它们的KV缓存。**然而，在现有系统中，内存共享是不可能的，因为序列的KV缓存存储在单独的连续空间中。**

为了解决上述限制，我们提出了PagedAttention，这是一种注意力算法，灵感来自于操作系统(OS)对内存碎片和共享的解决方案:带分页的虚拟内存。PagedAttention将请求的KV缓存划分为块，每个块可以包含注意键和固定数量令牌的值。在PagedAttention中，KV缓存的块不一定存储在连续的空间中。因此，我们可以像在OS的虚拟内存中那样更灵活地管理KV缓存:可以将块视为页面，令牌视为字节，请求视为进程。这种设计通过使用相对较小的块并按需分配来减轻内部碎片。此外，它消除了外部碎片，因为所有块都具有相同的大小。最后，它支持以块粒度、跨与相同请求关联的不同序列甚至跨不同请求共享内存。

在这项工作中，我们在PagedAttention的基础上构建了一个高吞吐量的分布式LLM服务引擎vLLM，它在KV高速缓存中实现了接近零的浪费。vLLM使用与PagedAttention共同设计的块级内存管理和抢占式请求调度。vLLM支持GPT[5]、OPT[62]、LLaMA[52]等流行的LLM，支持不同大小的LLM，包括超过单个GPU内存容量的LLM。我们对各种模型和工作负载的评估表明，与最先进的系统相比，vLLM将LLM服务吞吐量提高了2-4倍[31,60]，而完全不影响模型的准确性。对于更长的序列、更大的模型和更复杂的解码算法，改进更加明显(§4.3)。综上所述，我们做出了以下贡献:

* 我们确定了在服务LLM时内存分配方面的挑战，并量化了它们对服务性能的影响。
* 受操作系统中虚拟内存和分页的启发，提出了一种基于非连续分页内存中KV缓存的注意力算法PagedAttention。
* 我们设计并实现了vLLM，一个建立在PagedAttention之上的分布式LLM服务引擎。
* 我们在各种情况下评估了vLLM，并证明它大大优于以前的最先进的解决方案，如FasterTransformer[31]和Orca[60]。

# **2背景**

在本节中，我们描述了典型LLM的生成和服务过程，以及LLM服务中使用的迭代级调度。

## **2.1基于Transformer的大语言模型**

语言建模的任务是对标记列表(𝑥1，…，𝑥𝑛))的概率进行建模。、。由于语言具有自然的顺序排序，因此通常将整个序列的联合概率分解为条件概率的乘积(也称为自回归分解[3]):



Transformer[53]已经成为在大范围内对上述概率进行建模的事实上的标准架构。基于transformer的语言模型最重要的组件是它的自关注意力层。对于输入隐藏状态序列(𝑥1，…，𝑥n∈R𝑛×𝑑），自注意力层首先对每个位置向量进行线性变换，得到query、key和value向量:



然后，自注意层通过将某一位置的query向量与其之前的所有key向量相乘来计算关注分数𝑎ij，并计算输出𝑜i作为value向量的加权平均值:



除Eq.4中的计算外，Transformer模型中的所有其他组成部分，包括嵌入层、前馈层、层归一化[2]、剩余连接[22]、输出logit计算以及Eq.2中的query、key和value转换，都是按照位置独立应用的，形式为:yi=𝑓(xi)。



## **2.2LLM服务&自回归生成**

经过训练后，LLM通常被部署为条件生成服务(例如，API[34]或聊天机器人[19,35])。对LLM服务的请求提供了一个输入提示令牌列表(𝑥1，…，𝑥𝑛+1），LLM服务生成一个输出令牌列表(𝑥𝑛+1，…，𝑥𝑛+𝑇），根据式(1)，我们将提示符列表和输出列表的连接称为序列。

由于Eq.1中的分解，LLM只能逐个采样并生成新的token，并且每个新token的生成过程取决于该序列中所有先前的token，特别是它们的key向量和value向量。在这个顺序生成过程中，通常缓存现有token的key和value向量，以生成未来的token，称为KV缓存。注意，一个token的KV缓存依赖于它之前的所有token。这意味着同一token在序列中出现在不同位置的KV缓存将是不同的。

给定一个请求提示，LLM服务中的生成计算可以分解为两个阶段:

**prompt阶段**接受整个用户提示(𝑥1，…，𝑥𝑛)作为输入，并计算第一个新token出现的概率P(𝑥n+1 | 𝑥1，…，𝑥𝑛)。在此过程中，还生成了key向量𝑘1，…，𝑘𝑛和value向量𝑣1，…，𝑣𝑛。由于prompt token 𝑥1，…，𝑥𝑛都是已知的，提示阶段的计算可以使用矩阵乘法运算并行化。因此，这一阶段可以有效地利用GPU固有的并行性。

**自回归生成阶段**依次生成剩余的新token。在迭代𝑡,模型需要一个token 𝑥𝑛+𝑡作为输入和计算的概率𝑃(𝑥𝑛+𝑡+1 |𝑥1,…，𝑥𝑛+𝑡)，key向量为𝑘1，…，𝑘𝑛+𝑡和value向量𝑣1，…，𝑣𝑛+𝑡。请注意，位置1到𝑛+𝑡−1的key和value向量在以前的迭代中被缓存，在这次迭代中只计算新的key和value向量𝑘𝑛+𝑡和𝑣𝑛+𝑡。当序列达到最大长度(由用户指定或由LLM限制)或发出序列结束<eos>token时，此阶段完成。由于数据依赖性，不同迭代的计算不能并行化，通常采用矩阵-向量乘法，效率较低。因此，这一阶段严重地未充分利用GPU计算并成为内存限制，导致单个请求的大部分延迟。

## **2.2LLM的批处理技术**

通过批量处理多个请求，可以提高LLM服务的计算利用率。由于请求共享相同的模型权重，因此移动权重的开销在批处理请求中平摊，并且当批处理大小足够大时，可能会被计算开销所淹没。但是，由于两个原因，将请求批处理到LLM服务是非常重要的。首先，请求可能在不同的时间到达。幼稚的批处理策略要么让较早的请求等待较晚的请求，要么将传入的请求延迟到较早的请求完成，从而导致严重的排队延迟。其次，请求可能具有截然不同的输入和输出长度(图11)。直接的批处理技术将填充请求的输入和输出，以平衡它们的长度，从而浪费GPU计算和内存。

为了解决这个问题，人们提出了细粒度的批处理机制，如蜂窝批处理[16]和迭代级调度[60]。与在请求级别工作的传统方法不同，这些技术在迭代级别操作。在每次迭代之后，完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单个迭代后处理新请求，而不是等待整个批处理完成。此外，使用特殊的GPU内核，这些技术消除了填充输入和输出的需要。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。

# **3LLM服务中的内存挑战**

尽管细粒度批处理减少了计算浪费，并使请求能够以更灵活的方式进行批处理，但可以批处理的请求数量仍然受到GPU内存容量的限制，特别是分配给存储KV缓存的空间。换句话说，服务系统的吞吐量受内存限制。**克服这种内存限制需要解决内存管理中的以下挑战:**

**大KV缓存。**KV缓存大小随着请求数量的增加而快速增长。例如，对于13B参数的OPT模型[62]，**单个令牌的KV缓存需要800 KB的空间，计算为2(key和value向量)× 5120(隐藏状态大小)× 40(层数)× 2(每个FP16字节)。由于OPT可以生成多达2048个令牌的序列，因此存储一个请求的KV缓存所需的内存可能高达1.6 GB。**并发GPU的内存容量为几十GB。即使将所有可用内存分配给KV缓存，也只能容纳几十个请求。此外，低效的内存管理会进一步减小批处理大小，如图2所示。

**复杂的解码算法。**LLM服务提供了一系列解码算法供用户选择，每种算法对内存管理复杂性的影响各不相同。例如，当用户从单个输入提示请求多个随机样本时，程序建议中的典型用例[18]，提示部分的KV缓存可以共享，在我们的实验中(§6.3)，它占总KV缓存的12%，以最小化内存使用。另一方面，在自回归生成阶段，由于不同的样本结果及其对环境和位置的依赖，KV缓存应该保持不共享。KV缓存共享的程度取决于所采用的具体解码算法。在波束搜索[49]等更复杂的算法中，不同的请求波束可以共享其KV缓存的更大部分(高达55%的内存节省，参见§6.3)，并且共享模式随着解码过程的推进而发展。

**调度未知的输入和输出长度。**对LLM服务的请求在其输入和输出长度方面表现出可变性。这就要求内存管理系统能够适应各种提示长度。此外，随着解码时请求的输出长度增加，其KV缓存所需的内存也会扩展，并且可能耗尽可用内存以用于传入请求或正在生成的现有提示。系统需要做出调度决策，例如从GPU内存中删除或交换某些请求的KV缓存。

## **3.1现有系统中的内存管理**

由于当前深度学习框架中的大多数运算符[33,39]要求将张量存储在连续内存中，以前的LLM服务系统[31,60]也将一个请求的KV缓存存储为跨不同位置的连续张量。由于LLM的输出长度不可预测，因此它们根据请求的最大可能序列长度静态地为请求分配一块内存，而不考虑请求的实际输入或最终输出长度。

图3显示了两个请求:请求A的最大可能序列长度为2048，请求B的最大可能序列长度为512。**现有系统中的块预分配方案有三个主要的内存浪费来源:为未来token保留的插槽、由于过度供应潜在的最大序列长度而导致的内部碎片，以及来自内存分配器(如buddy分配器)的外部碎片。**外部碎片永远不会用于生成的token，这在服务请求之前是已知的。内部碎片也未被使用，但这只有在请求完成采样后才会实现。它们都是纯粹的内存浪费。虽然保留的内存最终会被使用，但是在整个请求期间保留这个空间，特别是当保留的空间很大时，会占用本来可以用来处理其他请求的空间。我们在图2中可视化了我们的实验中内存浪费的平均百分比，揭示了以前系统中的实际有效内存可以低至20.4%。

**虽然压缩[54]已经被提出作为一种潜在的碎片解决方案，但由于大量KV缓存，在性能敏感的LLM服务系统中执行压缩是不切实际的。即使使用了压缩，为每个请求预先分配的块空间也会阻止现有内存管理系统中特定于解码算法的内存共享。**

# **4方法**

在这项工作中，我们开发了一种新的注意力算法PagedAttention，并构建了一个LLM服务引擎vLLM，以解决§3中概述的挑战。vLLM的架构如图4所示。vLLM采用集中式调度程序来协调分布式GPU工作线程的执行。KV缓存管理器以分页方式有效地管理KV缓存，由PagedAttention启用。具体来说，KV缓存管理器通过集中调度程序发送的指令来管理GPU工作线程上的物理KV缓存。

接下来，我们在§4.1中描述PagedAttention算法。因此，我们分别在§4.2中展示KV缓存管理器的设计以及它如何促进§4.3中的PagedAttention。然后，我们展示了这种设计如何促进各种解码方法(§4.4)的有效内存管理，并处理可变长度的输入和输出序列(§4.5)。最后，我们展示了vLLM的系统设计如何在分布式设置中工作(第4.6节)。

## **4.1PagedAttention**

为了解决§3中的内存挑战，我们引入了PagedAttention，这是一种受操作系统中分页的经典思想启发的注意力算法[25]。与传统的注意力算法不同，PagedAttention允许在非连续的内存空间中存储连续的key和value。具体来说，PagedAttention将每个序列的KV缓存划分为KV块。每个块包含固定数量token的key和value向量，我们将其表示为KV块大小(B)。表示key块𝐾𝑗=(𝑘(𝑗−1)𝐵+1，…，𝑘𝑗𝐵)和value块𝑉𝑗=(𝑣(𝑗−1)𝐵+1，…，𝑣𝑗𝐵)。Eq.4中的注意力计算可以转化为如下逐块计算:



式中，Aij=(𝑎i,(𝑗−1)𝐵+1，…，𝑎i，𝑗𝐵)为第𝑗个KV块上注意力分数的行向量。

在注意力计算过程中，PagedAttention内核分别识别和提取不同的KV块。我们在图5中展示了一个PagedAttention的例子:key和value向量分布在三个块上，并且这三个块在物理内存上不是连续的。每次，内核将查询token(“forth”)的查询向量𝑞和块中的key向量𝐾𝑗(例如，块0的key向量“Four score and seven”)相乘，计算出注意力分数的变量𝑗，然后将变量𝑗与块中的value向量相乘，得到最终的注意力输出𝑜i。

总之，PagedAttention算法允许将KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。

## **4.2KV缓存管理器**

vLLM内存管理器背后的关键思想类似于操作系统中的虚拟内存[25]。操作系统将内存划分为固定大小的页面，并将用户程序的逻辑页面映射到物理页面。连续的逻辑页可以对应于非连续的物理内存页，允许用户程序访问内存，就好像它是连续的一样。此外，物理内存空间不需要提前完全预留，使操作系统可以根据需要动态分配物理页面。vLLM使用虚拟内存背后的思想来管理LLM服务中的KV缓存。通过PagedAttention，我们将KV缓存组织为固定大小的KV块，就像虚拟内存中的页面一样。

请求的KV缓存表示为一系列逻辑KV块，从左到右填充为生成的新token及其KV缓存。最后一个KV区块的未填充位置保留给后代使用。

在GPU工作器上，块引擎分配一个连续的GPU DRAM块，并将其划分为物理KV块(这也在CPU RAM上完成，用于交换;在§4.5)。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射。每个块表项记录一个逻辑块对应的物理块和填充位置的数量。分离逻辑和物理KV块允许vLLM动态增长KV缓存，而无需提前为所有位置保留它，这消除了现有系统中的大部分内存浪费，如图2所示。

## **4.3解码PagedAttention和vLLM**

接下来，我们通过一个示例，如图6所示，来演示vLLM如何在单个输入序列的解码过程中执行PagedAttention并管理内存:①在OS的虚拟内存中，vLLM最初不需要为可能生成的最大序列长度保留内存。相反，它只保留必要的KV块，以容纳在提示计算期间生成的KV缓存。在本例中，提示符有7个token，因此vLLM将前2个逻辑KV块(0和1)映射到2个物理KV块(分别为7和1)。在预填充步骤中，vLLM使用传统的自注意力算法(例如[13])生成提示符和第一个输出token的KV缓存。然后，vLLM将前4个token的KV缓存存储在逻辑块0中，并将随后的3个令牌存储在逻辑块1中。剩余的槽保留给后续的自回归生成阶段。②在第一个自回归解码步骤中，vLLM使用物理块7和1上的PagedAttention算法生成新的token。由于在最后一个逻辑块中仍然有一个槽可用，因此新生成的KV缓存存储在那里，并且块表的#filled记录被更新。③在第二步解码时，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在新的逻辑块中;vLLM为它分配一个新的物理块(物理块3)，并将这个映射存储在块表中。

全局而言，对于每次解码迭代，vLLM首先选择一组候选序列进行批处理(参见§4.5)，并为新需要的逻辑块分配物理块。然后，vLLM将当前迭代的所有输入令牌(即，提示阶段请求的所有token和生成阶段请求的最新token)连接为一个序列，并将其提供给LLM。在LLM的计算过程中，vLLM使用PagedAttention内核访问之前以逻辑KV块形式存储的KV缓存，并将新生成的KV缓存保存到物理KV块中。在KV块中存储多个token(块大小> 1)使PagedAttention内核能够跨多个位置并行处理KV缓存，从而增加硬件利用率并减少延迟。然而，更大的块大小也会增加内存碎片。我们在§7.2中研究了块大小的影响。

同样，当生成更多token及其KV缓存时，vLLM会动态地将新的物理块分配给逻辑块。由于所有的块都是从左到右填充的，并且只有在之前的所有块都已满时才分配新的物理块，因此vLLM将请求的所有内存浪费限制在一个块内，因此它可以有效地利用所有内存，如图2所示。这允许将更多请求放入内存中进行批处理，从而提高吞吐量。一旦一个请求完成了它的生成，它的KV块可以被释放来存储其他请求的KV缓存。在图7中，我们展示了一个vLLM管理两个序列的内存的示例。两个序列的逻辑块映射到GPU worker中块引擎保留的空间内的不同物理块。两个序列的相邻逻辑块不需要在物理GPU内存中连续，两个序列可以有效地利用物理块的空间。

## **4.4其他解码场景的应用**

§4.3展示了PagedAttention和vLLM如何处理基本的解码算法，例如贪婪解码和采样，将一个用户提示作为输入并生成单个输出序列。在许多成功的LLM应用程序中[18,34]，LLM服务必须提供更复杂的解码场景，表现出复杂的访问模式和更多的内存共享机会。在本节中，我们将展示vLLM对它们的一般适用性。

**并行采样。**在基于LLM的程序助手中[6,18]，LLM为单个输入提示生成多个采样输出;用户可以从各种候选输出中选择自己喜欢的输出。到目前为止，我们已经隐式地假设请求生成单个序列。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。

图8示出用于两个输出的并行解码的示例。由于两个输出共享相同的提示符，我们在提示阶段只为提示符状态的一个副本保留空间;两个序列的提示符的逻辑块映射到相同的物理块:两个序列的逻辑块0和1分别映射到物理块7和1。**由于单个物理块可以映射到多个逻辑块，因此我们为每个物理块引入一个引用计数。**在这种情况下，物理块7和1的引用计数都是2。在生成阶段，两个输出采样不同的输出token，需要单独存储KV缓存。对于需要通过多个序列修改的物理块，vLLM在块粒度上实现了一种写时复制机制，类似于操作系统虚拟内存中的写时复制技术(例如，当fork一个进程时)。具体来说，在图8中，当示例A1需要写入它的最后一个逻辑块(逻辑块1)时，vLLM识别到对应的物理块(物理块1)的引用计数大于1;它分配一个新的物理块(物理块3)，指示块引擎从物理块1复制信息，并将引用计数减少到1。接下来，当示例A2写入物理块1时，引用计数已经减少到1;因此A2直接将其新生成的KV缓存写入物理块1。

总之，vLLM支持跨多个输出样本共享用于存储提示的KV缓存的大部分空间，但最后的逻辑块除外，该逻辑块由写时复制机制管理。通过跨多个示例共享物理块，可以大大减少内存使用，特别是对于长输入提示。

**集束搜索。**在机器翻译等LLM任务中[59]，用户期望LLM输出的top-𝑘最合适的翻译。集束搜索[49]被广泛用于解码LLM最可能的输出序列，因为它降低了完全遍历样本空间的计算复杂度。该算法依赖于集束宽度参数𝑘，该参数决定了每一步保留的最佳候选数。在解码过程中，集束搜索通过考虑所有可能的标记来扩展集束中的每个候选序列，使用LLM计算它们各自的概率，并在𝑘·|V|候选序列中保留最可能的𝑘序列，其中|V|是词汇表大小。

与并行解码不同，集束搜索工具不仅共享初始提示块，还共享不同候选块，并且共享模式随着解码过程的推进而动态变化，类似于fork在操作系统中创建的进程树。图9显示了对于𝑘= 4的波束搜索示例，vLLM如何管理KV块。在用虚线表示的迭代之前，每个候选序列已经使用了4个完整的逻辑块。所有候选集束共享第一个块0(即提示符)。候选者3从第二部分开始偏离。候选者0-2共用前3个块，并在第四个块分开。在随后的迭代中，前4个可能的候选项都来自候选项1和2。由于原来的候选0和3不再是最优候选，它们的逻辑块被释放，相应的物理块的引用计数被减少。vLLM释放所有引用计数达到0的物理块(块2,4,5,8)，然后，vLLM分配新的物理块(块9-12)来存储来自新候选对象的新KV缓存。现在，所有候选者共享0、1、3块;候选物0和1共享区块6，候选物2和3进一步共享区块7。

以前的LLM服务系统需要在候选集束上频繁地复制KV缓存。例如，在图9所示的情况下，在虚线之后，候选3将需要复制候选2的KV缓存的大部分以继续生成。vLLM的物理块共享大大减少了这种频繁的内存复制开销。在vLLM中，不同候选集束的大部分块可以共享。只有当新生成的token位于旧的共享块中时，才应用写时复制机制，就像并行解码一样。这只涉及复制一个数据块。

**共享前缀。**通常，LLM用户提供任务的(长)描述，包括指令和示例输入输出，也称为系统提示符[36]。描述与实际任务输入连接起来，形成请求提示。LLM根据完整的提示符生成输出。图10显示了一个示例。此外，可以通过提示工程进一步调整共享前缀，以提高下游任务的准确性[26,27]。

**对于这种类型的应用程序，许多用户提示共享一个前缀，因此LLM服务提供商可以提前存储前缀的KV缓存，以减少在前缀上花费的冗余计算。在vLLM中，可以通过LLM服务提供者为一组预定义的共享前缀保留一组物理块来方便地实现这一点，正如操作系统如何跨进程处理共享库一样。带有共享前缀的用户输入提示符可以简单地将其逻辑块映射到缓存的物理块(最后一个块标记为写时复制)。**提示阶段的计算只需要在用户的任务输入上执行。

**混合解码方法。**前面讨论的解码方法表现出不同的内存共享和访问模式。尽管如此，vLLM促进了具有不同解码首选项的请求的同时处理，这是现有系统无法有效做到的。这是因为vLLM通过将逻辑块转换为物理块的公共映射层隐藏了不同序列之间的复杂内存共享。LLM及其执行内核只看到每个序列的物理块id列表，不需要处理跨序列的共享模式。与现有系统相比，这种方法扩大了具有不同采样要求的请求的批处理机会，最终提高了系统的总体吞吐量。

## **4.5调度和抢占**

当请求流量超过系统容量时，vLLM必须优先考虑请求子集。在vLLM中，我们对所有请求采用先到先服务(FCFS)调度策略，确保公平性并防止饥饿。当vLLM需要抢占请求时，它确保首先服务最早到达的请求，并首先抢占最新的请求。

LLM服务面临着一个独特的挑战:LLM的输入提示的长度可能会有很大的不同，并且结果的输出长度是未知的，这取决于输入提示和模型。随着请求数量及其输出的增长，vLLM可能会耗尽GPU的物理块来存储新生成的KV缓存。

在这种情况下，vLLM需要回答两个经典问题:(1)它应该驱逐哪些块?(2)如果再次需要，如何恢复被驱逐的块?通常，驱逐策略使用启发式方法来预测哪个块将在未来被访问得最远，并驱逐该块。因为在我们的例子中，我们知道序列的所有块都是一起被访问的，所以我们实现了一个全有或全无的驱逐策略，即，要么驱逐序列的所有块，要么不驱逐。此外，一个请求中的多个序列(例如，一个集束搜索请求中的集束候选序列)作为一个序列组进行分组调度。一个序列组中的序列总是被抢占或重新调度在一起，因为这些序列之间可能存在内存共享。为了回答第二个问题，即如何恢复被驱逐的块，我们考虑两种技术:

**交换。**这是大多数虚拟内存实现使用的经典技术，它将被驱逐的页面复制到磁盘上的交换空间。在本例中，我们将被驱逐的块复制到CPU内存中。如图4所示，除了GPU块分配器之外，vLLM还包含一个CPU块分配器，用于管理交换到CPU RAM的物理块。当vLLM为新token耗尽空闲物理块时，它会选择一组序列来驱逐并将它们的KV缓存传输到CPU。一旦它抢占了一个序列并驱逐了它的块，vLLM就会停止接受新的请求，直到所有被抢占的序列都被完成。一旦请求完成，它的块就从内存中释放出来，并将被抢占序列的块带回来继续处理该序列。请注意，在这种设计中，交换到CPU RAM的块数量永远不会超过GPU RAM中的总物理块数量，因此CPU RAM上的交换空间受到分配给KV缓存的GPU内存的限制。

**重新计算。**在这种情况下，当被抢占的序列被重新调度时，我们只需重新计算KV缓存。请注意，重新计算延迟可以显著低于原始延迟，因为在解码时生成的token可以与原始用户提示连接为一个新的提示—它们在所有位置的KV缓存可以在一个提示阶段迭代中生成。

交换和重计算的性能取决于CPU RAM和GPU内存之间的带宽以及GPU的计算能力。我们将在§7.3中检查交换和重计算的速度。

## **4.6分布式执行**

许多LLM的参数大小超过了单个GPU的容量[5,9]。因此，有必要将它们划分到分布式GPU上，并以模型并行的方式执行[28,63]。这需要能够处理分布式内存的内存管理器。vLLM通过支持Transform上广泛使用的Megatron-LM风格张量模型并行策略，在分布式设置中是有效的[47]。该策略遵循SPMD(单程序多数据)执行计划，其中线性层被划分以执行逐块矩阵乘法，GPU通过allreduce操作不断同步中间结果。具体来说，注意算子在注意头维度上被分割，每个SPMD过程在多头注意中负责注意头的一个子集。

我们观察到，即使模型并行执行，每个模型分片仍然处理相同的一组输入token，因此需要KV缓存用于相同的位置。因此，vLLM在集中式调度器中具有单个KV缓存管理器，如图4所示。不同的GPU工作者共享管理器，以及从逻辑块到物理块的映射。这种公共映射允许GPU工作人员使用调度程序为每个输入请求提供的物理块来执行模型。虽然每个GPU工作线程都有相同的物理块ID，但一个工作线程只为其相应的注意头存储一部分KV缓存。

在每个步骤中，调度器首先为批处理中的每个请求准备带有输入token ID的消息，并为每个请求准备块表。接下来，调度器将这个控制消息广播给GPU工作线程。然后，GPU工作程序开始使用输入token ID执行模型。在注意层，GPU处理器根据控制消息中的块表读取KV缓存。在执行过程中，GPU工作者与all-reduce通信原语同步中间结果，而不需要调度程序的协调，如[47]所示。最后，GPU工作器将这次迭代的采样token发送回调度器。总之，GPU处理单元不需要同步内存管理，因为他们只需要在每次解码迭代开始时接收所有内存管理信息以及步进输入。

# **5实现**

vLLM是一个端到端服务系统，采用FastAPI[15]前端和基于GPU的推理引擎。前端扩展了OpenAI API[34]接口，允许用户为每个请求定制采样参数，如最大序列长度和集束宽度𝑘。vLLM引擎是用8.5万行Python和2K行C++/CUDA代码编写的。我们在Python中开发控制相关组件，包括调度器和块管理器，同时为关键操作(如PagedAttention)开发自定义CUDA内核。对于模型执行器，我们使用PyTorch[39]和Transformers[58]实现了流行的LLM，如GPT[5]、OPT[62]和LLaMA[52]。我们使用NCCL[32]在分布式GPU worker之间进行张量通信。

## **5.1内核级优化**

由于PagedAttention引入了现有系统无法有效支持的内存访问模式，我们开发了几个GPU内核来优化它。(1)融合重塑和块写入。在每个Transformer层中，新的KV缓存被分割成块，重新塑造为块读取优化的内存布局，然后保存在块表指定的位置。为了最小化内核启动开销，我们将它们融合到一个内核中。(2)融合块阅读和注意力。我们采用FasterTransformer[31]中的注意力内核，根据块表读取KV缓存，并动态执行注意力操作。为了确保合并内存访问，我们分配了一个GPU warp来读取每个块。此外，我们还增加了对请求批处理中可变序列长度的支持。(3)熔块复制。由写时拷贝机制发出的块拷贝操作可以在不连续的块上操作。如果我们使用cudaMempyAsync API，这可能导致大量的小数据移动调用。为了减少开销，我们实现了一个内核，它将不同块的复制操作批处理到单个内核启动中。

## **5.2支持多种解码算法**

**vLLM使用三个关键方法实现各种解码算法:fork、append和free。fork方法从一个现有序列创建一个新序列。append方法向序列追加一个新标记。最后，free方法删除序列。例如，在并行采样中，vLLM使用fork方法从单个输入序列创建多个输出序列。然后在每次迭代中使用append向这些序列添加新的标记，并使用free删除满足停止条件的序列。vLLM在集束搜索和前缀共享中也采用了相同的策略。**我们相信结合这些方法也可以支持未来的解码算法。

# **6评价**

在本节中，我们将评估vLLM在各种工作负载下的性能。

## **6.1实验设置**

模型和服务器配置。我们使用具有13B、66B和175B参数的OPT[62]模型和具有13B参数的LLaMA[52]模型进行评估。从LLM排行榜上可以看出，13B和66B是LLM的常用尺寸[38]，而175B是著名的GPT-3[5]模型的尺寸。对于我们所有的实验，我们在谷歌云平台上使用带有NVIDIA A100 GPU的A2实例。详细的模型大小和服务器配置如表1所示。

**工作负载。**我们基于ShareGPT[51]和Alpaca[50]数据集合成工作负载，这些数据集包含真实LLM服务的输入和输出文本。ShareGPT数据集是用户与ChatGPT共享对话的集合[35]。Alpaca数据集是GPT3.5通过自指导生成的指令数据集[57]。我们对数据集进行标记，并使用它们的输入和输出长度来合成客户机请求。如图11所示，与Alpaca数据集相比，ShareGPT数据集的输入提示长8.4倍，输出平均长5.8倍，方差更高。由于这些数据集不包括时间戳，我们使用具有不同请求速率的泊松分布生成请求到达时间。

**基线1:FasterTransformer。**FasterTransformer[31]是一个针对延迟进行了高度优化的分布式推理引擎。由于FasterTransformer没有自己的调度程序，我们实现了一个带有动态批处理机制的自定义调度程序，类似于现有的服务系统，如Triton[30]。具体来说，我们根据GPU的内存容量为每个实验设置了尽可能大的最大批处理大小。调度程序占用了最早到达的请求的个数，并将批处理发送到fasttransformer进行处理。

基线2:Orca。Orca[60]是最先进的LLM服务系统，针对吞吐量进行了优化。由于Orca不能公开使用，我们实现了自己的Orca版本。我们假设Orca使用伙伴分配算法来确定存储KV缓存的内存地址。基于Orca为请求输出预留了多少空间，我们实现了三个版本:

* Orca(Oracle)。我们假设系统知道将为请求实际生成的输出的长度。这说明了Orca的性能上界，这在实际中是不可实现的。

* Orca(po2)。我们假设系统为输出预留了最多2倍的空间。例如，如果真实的输出长度为25，则为输出保留32个位置。
* Orca(Max)。我们假设系统总是保留空间到模型的最大序列长度，即2048个令牌。

**关键指标。**我们专注于服务吞吐量。具体来说，使用具有不同请求速率的工作负载，我们测量了系统的归一化延迟，即每个请求的端到端延迟的平均值除以其输出长度，如Orca[60]所示。高吞吐量服务系统应该在高请求率下保持较低的规范化延迟。对于大多数实验，我们用1小时的跟踪来评估系统。作为例外，由于成本限制，我们对OPT-175B模型使用15分钟的跟踪。

## **6.2基本抽样**

我们在三个模型和两个数据集上使用基本采样(每个请求一个样本)来评估vLLM的性能。图12的第一行显示了ShareGPT数据集上的结果。曲线表明，随着请求速率的增加，延迟最初以渐进的速度增加，但随后突然激增。这可以归因于这样一个事实，即当请求速率超过服务系统的容量时，队列长度继续无限增长，请求的延迟也会无限增长。

在ShareGPT数据集上，vLLM可以维持1.7×-2.7×比Orca (Oracle)更高的请求率，2.7×-8×比Orca (Max)更高的请求率，同时保持相似的延迟。这是因为vLLM的PagedAttention可以有效地管理内存使用，因此可以批处理比Orca更多的请求。例如，如图13a所示，对于OPT-13B, vLLM同时处理的请求数比Orca (Oracle)多2.2倍，比Orca (Max)多4.3倍。与FasterTransformer相比，vLLM可以维持高达22倍的高请求率，因为FasterTransformer不使用细粒度调度机制，并且不像Orca (Max)那样有效地管理内存。

图12和图13b的第二行显示了在Alpaca数据集上的结果，它遵循与ShareGPT数据集类似的趋势。一个例外是图12(f)，其中vLLM相对于Orca (Oracle)和Orca (Pow2)的优势不那么明显。这是因为OPT-175B的模型和服务器配置(表1)允许使用大的GPU内存空间来存储KV缓存，而Alpaca数据集具有短序列。在这种设置中，Orca(Oracle)和Orca(Pow2)也可以批处理大量请求，尽管它们的内存管理效率很低。因此，系统的性能受到计算而不是内存的限制。

## **6.3并行采样和集束搜索**

我们用两种流行的采样方法:并行采样和集束搜索来评估PagedAttention中内存共享的有效性。在并行采样中，一个请求中的所有并行序列可以共享KV缓存。如图14第一行所示，由于需要采样的序列数量更多，vLLM比Orca基线带来了更大的改进。同样，图14第二行为不同集束宽度下的集束搜索结果。由于集束搜索允许更多的共享，所以vLLM展示了更大的性能优势。在OPT-13B和Alpaca数据集上，vLLM对Orca (Oracle)的改进从基本采样的1.3倍提高到宽度为6的集束搜索的2.3倍。

图15绘制了内存节省量，**计算方法是通过共享节省的块数除以不共享的总块数**。我们发现并行采样节省了6.1% - 9.8%的内存，波束搜索节省了37.6% - 55.2%的内存。在ShareGPT数据集的相同实验中，我们看到并行采样节省了16.2% - 30.5%的内存，集束搜索节省了44.3% - 66.3%的内存。

## **6.4共享前缀**

我们探讨了vLLM在不同输入提示符共享前缀的情况下的有效性，如图10所示。对于模型，我们使用LLaMA-13B[52]，它是多语言的。对于工作负载，我们使用WMT16[4]的英语到德语翻译数据集，并合成两个前缀，其中包括一条指令和一些翻译示例。第一个前缀包括一个例子(即one-shot)，而另一个前缀包括5个例子(即few-shot)。如图16 (a)所示，共享一次前缀时，vLLM的吞吐量比Orca(Oracle)高1.67倍。此外，当共享更多的示例时(图16 (b))， vLLM的吞吐量比Orca(Oracle)高3.58倍。

## **6.5聊天机器人**

聊天机器人[8,19,35]是LLM最重要的应用之一。为了实现聊天机器人，我们让模型通过将聊天历史记录和最后一个用户查询连接到提示中来生成响应。我们使用ShareGPT数据集综合了聊天历史和用户查询。由于OPT-13B模型的上下文长度有限，我们将提示符削减到最后1024个令牌，并让模型最多生成1024个令牌。我们不会在不同的会话轮之间存储KV缓存，因为这样做会占用会话轮之间其他请求的空间。

图17显示，与三个Orca基线相比，vLLM可以维持高2倍的请求率。由于ShareGPT数据集包含许多长对话，因此大多数请求的输入提示都有1024个令牌。由于使用了伙伴分配算法，Orca基线为请求输出保留了1024个令牌的空间，而不管它们如何预测输出长度。出于这个原因，三条Orca基线的行为相似。相比之下，vLLM可以有效地处理长提示，因为PagedAttention解决了内存碎片和保留的问题。

# **7消融实验**

在本节中，我们将研究vLLM的各个方面，并通过消融实验评估我们所做的设计选择。

## **7.1内核微基准测试**

PagedAttention中的动态块映射影响涉及存储KV缓存的GPU操作的性能，即块读/写和注意。与现有系统相比，我们的GPU内核(§5)涉及访问块表，执行额外分支和处理可变序列长度的额外开销。如图18a所示，与高度优化的FasterTransformer实现相比，这导致了20-26%的高注意力内核延迟。我们认为开销很小，因为它只影响注意操作符，而不影响模型中的其他操作符，例如Linear。尽管有开销，PagedAttention使vLLM在端到端性能上明显优于FasterTransformer(§6)。

## **7.2块大小的影响**

**块大小的选择对vLLM的性能有很大的影响。如果块大小太小，vLLM可能无法充分利用GPU的并行性来读取和处理KV缓存。如果块大小太大，则内部碎片增加，共享概率降低。**

在图18b中，我们在固定请求率下使用基本采样的ShareGPT和Alpaca跟踪来评估不同块大小的vLLM的性能。在ShareGPT跟踪中，块大小从16到128可以获得最佳性能。在Alpaca跟踪中，虽然块大小为16和32可以很好地工作，但更大的块大小会显著降低性能，因为序列变得比块大小短。在实践中，我们发现块大小16足够大，可以有效地利用GPU，也足够小，可以在大多数工作负载中避免显著的内部碎片。因此，vLLM将其默认块大小设置为16。

## **7.3比较重计算和交换**

vLLM支持重计算和交换作为其恢复机制。为了理解这两种方法之间的权衡，我们评估了它们的端到端性能，并对它们的开销进行了微基准测试，如图19所示。我们的结果表明，在小块大小的情况下，交换会导致过多的开销。这是因为小块大小通常会导致CPU和GPU之间的大量小数据传输，这限制了有效的PCIe带宽。相比之下，重新计算的开销在不同块大小的情况下保持不变，因为重新计算不利用KV块。因此，当块大小较小时，重新计算更有效，而当块大小较大时，交换更有效，尽管重新计算开销永远不会超过交换延迟的20%。对于从16到64的中等块大小，这两种方法表现出相当的端到端性能。

# **8讨论**

**将虚拟内存和分页技术应用于其他GPU工作负载。**虚拟内存和分页的思想对于管理LLM服务中的KV缓存是有效的，因为工作负载需要动态内存分配(因为输出长度不是先验的)，并且它的性能受GPU内存容量的约束。然而，这通常并不适用于每个GPU工作负载。例如，在深度神经网络训练中，张量形状通常是静态的，因此可以提前优化内存分配。例如，在服务非LLM的DNN时，内存效率的增加可能不会导致任何性能改进，因为性能主要与计算有关。在这种情况下，引入vLLM的技术可能会降低性能，因为间接内存和非连续块内存会带来额外的开销。但是，我们很高兴看到vLLM的技术被应用到具有与LLM服务相似属性的其他工作负载上。

**应用虚拟内存和分页时LLM特有的优化。**vLLM通过利用特定于应用程序的语义重新解释和增强了虚拟内存和分页的概念。一个例子是vLLM的全有或全无交换策略，它利用了这样一个事实，即处理请求需要将其所有相应的token状态存储在GPU内存中。另一个例子是重新计算的方法来恢复被驱逐的块，这在操作系统中是不可行的。此外，vLLM通过将内存访问操作的GPU内核与其他操作(如注意力)的GPU内核融合在一起，减轻了分页中间接的内存开销。

# **9相关工作**

**通用模型服务系统。**模型服务近年来一直是一个活跃的研究领域，提出了许多系统来解决深度学习模型部署的各个方面。Clipper[11]、TensorFlow Serving[33]、Nexus[45]、interline[10]和Clockwork[20]是一些较早的通用模型服务系统。他们研究为单个或多个模型服务的批处理、缓存、放置和调度。最近，DVABatch[12]引入了多入口多出口批处理。REEF[21]和Shepherd[61]建议优先服务。AlpaServe[28]利用模型并行性进行统计复用。然而，这些通用系统没有考虑到LLM推理的自回归性质和token状态，从而错失了优化的机会。

**Transformer专用服务系统。**由于transformer体系结构的重要性，为它开发了许多专门的服务系统。这些系统利用GPU内核优化[1,29,31,56]、高级批处理机制[14,60]、模型并行性[1,41,60]和参数共享[64]来实现高效服务。其中，Orca[60]与我们的方法最为相关。

**与Orca比较。**Orca中的迭代级调度[60]和vLLM中的PagedAttention是互补的技术:虽然两个系统都旨在提高GPU利用率，从而提高LLM服务的吞吐量，但Orca通过调度和交错请求来实现这一目标，以便可以并行处理更多请求，而vLLM通过增加内存利用率来实现这一目标，以便更多请求的工作集适合内存。通过减少内存碎片和启用共享，vLLM可以并行地批量运行更多的请求，与Orca相比，速度提高了2-4倍。实际上，像Orca中那样的细粒度调度和请求交错使得内存管理更具挑战性，使得vLLM中提出的技术更加重要。

**内存优化。**加速器的计算能力和内存容量之间的差距越来越大，使得内存成为训练和推理的瓶颈。利用交换[23,42,55]、重计算[7,24]以及它们的组合[40]来降低训练的峰值内存。值得注意的是，FlexGen[46]研究了如何在有限的GPU内存下交换权重和令牌状态以进行LLM推理，但它并不针对在线服务设置。OLLA[48]优化了张量的生命周期和位置，以减少碎片，但它不做细粒度的块级管理或在线服务。FlashAttention[13]采用平铺和内核优化来降低注意力计算的峰值内存，降低I/O成本。本文介绍了在线服务环境下的块级内存管理的新思想。

# **10总结**

本文提出了一种新的注意力算法PagedAttention，该算法允许将注意力键和值存储在非连续的分页内存中，并提出了一种基于PagedAttention的高吞吐量LLM服务系统vLLM，该系统具有高效的内存管理功能。受操作系统的启发，我们演示了如何将现有的技术，如虚拟内存和写时复制，用于有效地管理KV缓存和处理LLM服务中的各种解码算法。我们的实验表明，与最先进的系统相比，vLLM实现了2-4倍的吞吐量改进。