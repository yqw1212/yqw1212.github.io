---
layout: post
title:  Reinforcement learning
date:   2020-11-06 00:01:01 +0300
image:  2020-11-06-winter.jpg
tags:   [MachineLearning]
---

强化学习就是程序或智能体（ agent ）通过与环境不断地进行交互学习一个从环境到动作的映射，学习的目标就是使累计回报最大化。
强化学习是一种试错学习，因其在各种状态（环境）下需要尽量尝试所有可以选择的动作，通过环境给出的反馈（即奖励）来判断动作的优劣，最终获得环境和最优动作的映射关系（即策略）。

### 马尔可夫决策过程

马尔可夫决策过程（Markov Decision Process）通常用来描述一个强化学习问题。
智能体agent根据当前对环境的观察、采取、动作获得环境的反馈，并使环境发生改变的循环过程。

### Q-learning

在现实的强化学习任务中，环境的转移概率、奖励函数往往很难得知，甚至很难得知环境中有多少状态。若学习算法不再依赖于环境建模，则称为免模型学习，蒙特卡洛强化学习就是其中的一种。
蒙特卡洛强化学习使用多次采样，然后求取平均累计奖赏作为期望累计奖赏的近似。

蒙特卡洛强化学习：直接对状态动作值函数Q(s,a）进行估计，每采样一条轨迹，就根据轨迹中的所有“状态动作”利用下面的公式对来对值函数进行更新。
$$
Q(s,a)=\frac{Q(s,a)*count(s,a)+R}{count(s,a)+1}
$$

每次采样更新完所有的“状态动作”对所对应的Q(s,a)，就需要更新采样策略π。但由于策略可能是确定性的，即一个状态对应一个动作，多次采样可能获得相同的采样轨迹，因此需要借助ε贪心策略：
$$
\begin{eqnarray}
𝝅(𝒔,𝒂)=
\begin{cases}
argmax_aQ(s,a)    & 以概率1−𝜺 \\
随机从A中选取动作    & 以概率𝜺
\end{cases}
\end{eqnarray}\\
$$
蒙特卡洛强化学习算法需要采样一个完整的轨迹来更新值函数，效率较低，此外该算法没有充分利用强化学习任务的序贯决策结构。
Q-learning算法结合了动态规划与蒙特卡洛方法的思想，使得学习更加高效。

#### Q-learning 算法

……

## 深度强化学习（DRL）

* 传统强化学习：真实环境中的状态数目过多，求解困难。
* 深度强化学习：将深度学习和强化学习结合在一起，通过深度神经网络直接学习环境（或观察）与状态动作值函数Q(s,a）之间的映射关系，简化问题的求解。

### Deep Q Network（DQN）

Deep Q Network （DQN ）：是将神经网络 (neural  network）和Q-learning结合，利用神经网络近似模拟函数 Q(s,a)，输入是问题的状态（e.g., 图形），输出是每个动作a对应的Q值，然后依据Q值大小选择对应状态执行的动作，以完成控制。
神经网络的参数：应用监督学习完成

#### 学习流程：

* 状态 s 输入，获得所有动作对应的Q值Q(s,a)。
* 选择对应Q值最大的动作a′并执行；
* 执行后环境发生改变，并能够获得环境的奖励 r
* 利用奖励r更新Q(s,a')——强化学习
  利用新的Q(s,a')更新网络参数——监督学习

-------------------

2013年，Deep Mind团队在NIPS上发表《Playing Atari with Deep Reinforcement Learning》一文，在该文中首次提出Deep Reinforcement Learning一词，并且提出DQN（Deep Q Network）算法，实现了从纯图像输入完全通过学习来玩Atari游戏。

## 自主学习Flappy Bird游戏

目标：使用深度强化学习方法自主学习Flappy Bird游戏策略，达到甚至超过人类玩家的水平。

技术路线：Deep Q Network
使用工具：tensorflow + pygame + cv2

OpenCV 是一个开源的跨平台的计算机视觉库，实现了大量的图像处理和计算机视觉方面的通用算法。
本实验采用OpenCv对采集的游戏画面进行预处理。

测试tensorflow是否安装成功

```assembly
import tensorflow as tf

hello = tf.constant('Hello tensorfolw')
sess = tf.Session()
print(sess.run(hello))
```

### 程序与模拟器交互

训练过程也就是神经网络（agent）不断与游戏模拟器（Environment）进行交互，通过模拟器获得状态，给出动作，改变模拟器中的状态，获得反馈，依据反馈更新策略的过程。

训练过程过程主要分为以下三个阶段：

* 观察期（OBSERVE）：程序与模拟器进行交互，随机给出动作，获取模拟器中的状态，将状态转移过程存放在D（Replay Memory）中。

  * 打开游戏模拟器，不执行跳跃动作，获取游戏的初始状态。
  * 根据ε贪心策略获得一个动作（由于神经网络参数也是随机初始化的，在本阶段参数也不会进行更新，所以统称为随机动作），并根据迭代次数减小ε的大小。
  * 由模拟器执行选择的动作，能够返回新的状态和反馈奖励。
  * 将上一状态s，动作a，新状态s‘，反馈r组装成（s，a，s‘，r）放进Replay Memory中用作以后的参数更新。
  * 根据新的状态s‘，根据ε贪心策略选择下一步执行的动作，周而复始，直至迭代次数到达探索期。

* 探索期（EXPLORE）：程序与模拟器交互的过程中，依据Replay Memory中存储的历史信息更新网络参数，并随训练过程降低随机探索率ε。

  探索期与观察期的唯一区别在于会根据抽样对网络参数进行更新。

  * 迭代次数达到一定数目，进入探索期，根据当前状态s，使用ε贪心策略选择一个动作（可以是随机动作或者由神经网络选择动作），并根据迭代次数减小ε的值。
  * 由模拟器执行选择的动作，能够返回新的状态和反馈奖励。
  * 将上一状态s，动作a，新状态s‘，反馈r组装成（s，a，s’，r）放进Replay Memory中用作参数更新。
  * 从Replay Memory中抽取一定量的样本，对神经网络的参数进行更新。
  * 根据新的状态s‘，根据ε贪心策略选择下一步执行的动作，周而复始，直至迭代次数到达训练期

* 训练期（TRAIN） ：ε 已经很小，不再发生改变，网络参数随着训练过程不断趋于稳定。

  本阶段跟探索期的过程相同，只是在迭代过程中不再修改ε的值。

------------------

#### 游戏模拟器

使用Python 的Pygame模块完成的Flappy Bird游戏程序，为了配合训练过程，在原有的游戏程序基
础上进行了修改。参考以下网址查看游戏源码：

https://github.com/sourabhv/FlapPyBird

训练过程中使用连续4帧图像作为一个状态s，用于神经网络的输入。

#### 动作选择模块

为ε贪心策略的简单应用，以概率ε随机从动作空间A中选择动作，以1-ε概率依靠神经网络的输出选择动作

#### 深度神经网络CNN

DQN：用卷积神经网络对游戏画面进行特征提取，这个步骤可以理解为对状态的提取。
卷积神经网络(CNN)：右侧展示卷积操作。

卷积核：这里的卷积核指的就是移动中3×3大小的矩阵。

#### 卷积操作

使用卷积核与数据进行对应位置的乘积并加和，不断移动卷积核生成卷积后的特征。

#### CNN池化操作

对卷积的结果进行操作。最常用的是最大池化操作，即从卷积结果中挑出最大值。

#### 卷积神经网络

把Image矩阵中的每个元素当做一个神经元，那么卷积核就相当于输入神经元和输出神经元之间的链接权重，由此构建而成的网络被称作卷积神经网络。

本实验中使用的深度神经网络结构就是多个卷积操作和池化操作的累加。

#### 深度神经网络

* 对采集的4张原始图像进行预处理，得到80\*80\*4大小的矩阵；
* 使用32个8\*8\*4大小步长4的卷积核对以上矩阵进行卷积，得到20\*20\*32大小的矩阵；注：在tensorflow中使用4维向量表示卷积核[输入通道数，高度，宽度，输出通道数]，对应于上面的[4,8,8,32]，可以理解为32个8\*8\*4大小的卷积核；
* 对以上矩阵进行不重叠的池化操作，池化窗口为2\*2大小，步长为2，得到10\*10\*32大小的矩阵；
* 使用64个4\*4\*32大小步长为2的卷积核对以上矩阵进行卷积，得到5\*5\*64的矩阵；
* 使用64个3\*3\*64大小步长为1的卷积核对以上矩阵进行卷积，得到5\*5\*64的矩阵；
* 将输出的5\*5\*64大小的数组进行reshape，得到1*1600大小的矩阵；
* 在之后添加一个全连接层，神经元个数为 512.
* 最后一层也是一个全连接层，神经元个数为2，对应的是就是两个动作的动作值函数；

通过获得输入s，神经网络就能够：

* 输出Q(s,a1)和Q(s,a2)比较两个值的大小，就能够评判采用动作a1和a2的优劣，从而选择要采取的动作
* 在选择并执行完采用的动作后，模拟器会更新状态并返回回报值，然后将这个状态转移过程存储进D进行采样 更新网络参数 。

## tensorflow基本使用

理解TensorFlow

* 使用图(graph)来表示计算任务；
* 在被称之为会话(Session)的上下文context中执行图；
* 使用tensor（张量）表示数据
* 通过变量(Variable)维护状态；
* 使用feed和fetch可以为任意的操作(arbitrary operation)赋值或者从其中获取数据。

TensorFlow是一个编程系统，使用图来表示计算任务。图中的节点被称作op(Operation)。op可以获得0个或多个 tensor，产生0个或多个tensor。每个tensor是一个类型化的多维数组。例如：可以将一组图像集表示成一个四维的浮点数组，四个维度分别是[batch，height，weight，channels] 。
图（graph）描述了计算的过程。为了进行计算，图必须在会话中启动，会话负责将图中的op分发到CPU或GPU 上进行计算，然后将产生的tensor返回。在Python中，tensor就是numpy.ndarray对象。

TensorFlow程序通常被组织成两个阶段：构建阶段和执行阶段。
构建阶段：op的执行顺序被描述成一个图；
执行阶段：使用会话执行图中的op 。
例如：通常在构建阶段创建一个图来表示神经网络，在执行阶段反复执行图中的op训练神经网络。

```assembly
>>>import tensorflow as tf #导入tensorflow库
>>>mat1 = tf.constant([[3., 3 .]]) #创建一个 1*2 的矩阵
>>>mat2 = tf.constant([[2.],[2 ]]) #创建一个 2*1 的矩阵
>>>product = tf.matmul(mat1 , mat2) #创建op执行两个矩阵的乘法
>>>sess = tf.Session() #启动默认图
>>>res = sess.run(product) #在默认图中执行 op 操作
>>>print(res) #输出乘积结果
[[ 12. ]]
>>>sess.close() #关闭session
```

#### 交互式会话（InteractiveSession）
为了方便使用Ipython之类的Python交互环境，可以使用交互式会话（InteractiveSession）来代替Session，使用类似Tensor.run()和Operation.eval()来代替Session.run()，避免使用一个变量来持有会话。

```assembly
>>>import tensorflow as tf #导入tensorflow库
>>>sess = tf.InteractiveSession() #创建交互式会话
>>>a = tf.Variable([1.0, 2.0]) #创建变量数组
>>>b = tf.constant([3.0, 4.0]) #创建常量数组
>>>sess.run(tf.global_variables_initializer()) #变量初始化
>>>res = tf.add(a, b) #创建加法操作
>>>print( res.eval()) #执行操作并输出结果
[4. 5.]
```

#### Feed操作
前面的例子中，数据均以变量或常量的形式进行存储。Tensorflow还提供了 Feed 机制，该机制可以临时替代图中任意操作中的tensor。最常见的用例是使用tf.placeholder()创建占位符，相当于是作为图中的输入，然后使用Feed机制向图中占位符提供数据进行计算.

```assembly
>>>import tensorflow as tf #导入tensorflow库
>>>sess = tf.InteractiveSession() #创建交互式会话
>>>input1 = tf.placeholder(tf.float32) #创建占位符
>>>input2 = tf.placeholder(tf.float32) #创建占位符
>>>res = tf.mul(input1, input2) #创建乘法操作
>>>res.eval( feed_dict ={input1:[7.], input2:[2]} ) #求值
array([14.], dtype = float32)
```

### 自主学习flappy bird实例程序编写